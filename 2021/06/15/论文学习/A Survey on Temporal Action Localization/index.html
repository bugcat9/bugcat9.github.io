<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
<meta name="google-site-verification" content="h6mqk4DHw-91R-MekBrLgCwD8GSHlkGduz8ltdkIfFI" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">



<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"bugcat.top","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="[toc] A Survey on Temporal Action Localization 摘要:在计算机视觉中，时间动作定位是视频理解中最关键也是最具挑战性的问题之一。由于其广泛的应用，近年来引起了广泛的关注日常生活应用。时间动作定位技术已经取得了很大的进展，特别是最近深度学习的发展。而且在未裁剪的情况下，现在需要更多的时间动作定位视频。在这篇论文中，我们的目标是调查最新的技术和模型的视频时间">
<meta property="og:type" content="article">
<meta property="og:title" content="A Survey on Temporal Action Localization">
<meta property="og:url" content="https://bugcat.top/2021/06/15/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/A%20Survey%20on%20Temporal%20Action%20Localization/index.html">
<meta property="og:site_name" content="bugCat&#39;s Blog">
<meta property="og:description" content="[toc] A Survey on Temporal Action Localization 摘要:在计算机视觉中，时间动作定位是视频理解中最关键也是最具挑战性的问题之一。由于其广泛的应用，近年来引起了广泛的关注日常生活应用。时间动作定位技术已经取得了很大的进展，特别是最近深度学习的发展。而且在未裁剪的情况下，现在需要更多的时间动作定位视频。在这篇论文中，我们的目标是调查最新的技术和模型的视频时间">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201027193624091.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201027201856154.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201114104053741.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201114213744116.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201114214140041.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201115121823022.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201115130532292.png">
<meta property="article:published_time" content="2021-06-15T22:00:00.000Z">
<meta property="article:modified_time" content="2024-10-20T13:34:26.732Z">
<meta property="article:author" content="bugCat">
<meta property="article:tag" content="时序动作定位">
<meta property="article:tag" content="动作检测">
<meta property="article:tag" content="综述">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201027193624091.png">

<link rel="canonical" href="https://bugcat.top/2021/06/15/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/A%20Survey%20on%20Temporal%20Action%20Localization/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>A Survey on Temporal Action Localization | bugCat's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="bugCat's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">bugCat's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">毛的感情的程序猿</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/bugcat9" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://bugcat.top/2021/06/15/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/A%20Survey%20on%20Temporal%20Action%20Localization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/avatar/zhouning.png">
      <meta itemprop="name" content="bugCat">
      <meta itemprop="description" content="啥都不会">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="bugCat's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Survey on Temporal Action Localization
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-15 22:00:00" itemprop="dateCreated datePublished" datetime="2021-06-15T22:00:00+00:00">2021-06-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-10-20 13:34:26" itemprop="dateModified" datetime="2024-10-20T13:34:26+00:00">2024-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">论文学习</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>13k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>12 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>[toc]</p>
<h1 id="A-Survey-on-Temporal-Action-Localization"><a href="#A-Survey-on-Temporal-Action-Localization" class="headerlink" title="A Survey on Temporal Action Localization"></a>A Survey on Temporal Action Localization</h1><blockquote>
<p><strong>摘要:</strong>在计算机视觉中，时间动作定位是视频理解中最关键也是最具挑战性的问题之一。由于其广泛的应用，近年来引起了广泛的关注日常生活应用。时间动作定位技术已经取得了很大的进展，特别是最近深度学习的发展。而且在未裁剪的情况下，现在需要更多的时间动作定位视频。在这篇论文中，我们的目标是调查最新的技术和模型的视频时间行动定位。主要包括相关技术、一些基准数据集和评价时间动作定位的度量。此外，我们从两个方面总结了时间动作定位各方面:全监督学习和弱监督学习。并列举了几部具有代表性的作品并比较他们各自的表现。最后，对其进行了深入分析，并提出了发展前景研究方向，并总结调查。</p>
</blockquote>
<p><strong>关键词</strong>：动作检测，计算机视觉，全监督学习，时间动作定位，弱监督学习。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h2><p>随着视频数量急剧的增长，视频理解成为了计算机视觉领域的一个热点问题和具有挑战性的方向。这个视频理解发个信包括许多子研究方向，包括在夏威夷，被CVPR举办的ActivityNet 挑战2017，这个网络一共提出了5个任务。</p>
<ul>
<li>未裁剪的视频分类(Untrimmed Video Classification )</li>
<li>裁剪后的行动识别( Trimmed Action Recognition)</li>
<li>时间动作检测( Temporal Action Proposals)</li>
<li>时间动作定位(Temporal Action Localization)</li>
<li>视频中密集的字幕事件(Dense-Captioning Events in Videos)</li>
</ul>
<p>在最近的调查中，我们关注的是时间动作定位，也就是上面列出的第四个。它需要检测包含目标动作的时间间隔。对于长时间的<strong>未裁剪的视频</strong>，时间动作定位主要解决两个任务，识别和定位。特别是,a)动作发生的起始时间和终止时间,b)每个提案的类别是什么属于(如挥手、爬山、扣篮)。当然，一个视频可能包含一个或多个行动剪辑(action clips),所以时间动作定位是要开发模型和技术来提供计算机视觉应用所需要的最基本的信息:动作是什么，动作什么时候发生?我们将这个任务作为动作定位，或时间动作定位，或动作检测。</p>
<p>虽然动作识别和动作本地化都是视频理解里面很重要的任务，但是时间动作定位比动作识别更加具有挑战性。动作识别和动作定位的关系和图像检测类似于图像识别和图像检测。但是由于时间连续信息(temporal series information),时间动作定位比图像检测更见困难。困难主要来自以下几个方面：<strong>a)</strong>时间信息，由于1维时间连续信息，时间动作定位不能使用静态图片信息，它必须结合时间连续信息。<strong>b)</strong>与目标检测不同的是，边界对象通常是非常清晰的，所以我们可以为对象标记一个更清晰的边界框。然而，可能没有关于动作的确切时间范围合理定义，所以，不可能给一个动作开始和结束的准确边界。<strong>c)</strong>大的时间跨度，时间动作片段的跨度可以是非常大的，比如，挥手可能只几秒钟但是攀岩和骑自行车能够持续十几秒。它们时间跨度在长度上的不同，是的提取检测(extract proposals)很困难。另外，在开放的环境当中，这里也又许多问题，例如多尺度，多目标和相机移动。</p>
<p>时间动作定位非常贴近我们的生活，它具有广泛的应用前景和社会价值在视频概况(video summarization)、公共视频监控、技能评估和日常生活安全。所以它在最最近几年得到了广泛的关注。与“动作检测”有关的出版物总数约为324127份，近二十年来包括书籍、期刊、论文、会议论文、专利和一些科技成果。下面我们主要分析出版学术和回忆论文的趋势动作检测，如同<strong>图1</strong>所示</p>
<p><img src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201027193624091.png" alt=""></p>
<p>本调查旨在帮助对时态动作本地化感兴趣的初学者。它提供一个概括动作定位的方法和最新进展，本文余下部分组织如下。</p>
<ul>
<li>第二节概述相关技术。</li>
<li>第三节介绍基本的时间动作定位数据集</li>
<li>第四节描述模型的性能评估指标</li>
<li>第五节从全监督和弱监督两方面，提供一个时间动作定位模型和方法的概述</li>
<li>第六节讨论现在的挑战和建议未来的方向</li>
<li>第七节总结本论文</li>
</ul>
<span id="more"></span>
<h2 id="2-相关技术"><a href="#2-相关技术" class="headerlink" title="2.相关技术"></a>2.相关技术</h2><p>因为最近时间本地化已经成为了一个活跃的研究领域，许多解决此问题的不同的方法被提出。虽然动作检测已经研究了许多年，但是它仍处于实验室数据集的测试阶段，没有实际的实用性和工业化。理解视频中动作发生的时间和内容是非常具有挑战性的。可以看出，目前对于这个任务仍然没有健壮的解决方案。在本节中，我们将回顾时间动作定位的相关技术。</p>
<p>众所周知，视频特征表示可以为视频动作提供有用的信息，并且很多实验已经被做了。在过去的二十年，众所周知特征提取的进展一般经历了两个重要的历史时期。一个是传统的动作检测阶段在2014年之前，另一个时期时深度学习阶段，在2014年之后。时间线框架如<strong>图2</strong>所示</p>
<p><img src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201027201856154.png" alt="image-20201027201856154"></p>
<p>在深度学习阶段，它们主要被分为两种类型框架：“两阶段检测(‘two-stage detection)”和“一阶段检测（one-stage detection）”。特别，前面一种依赖“检测和分类”(proposal-then-classification)范式，这是一个主流方法，后一种同时检测和分类,所以我们称之为一级检测</p>
<h3 id="A-传统的方法-TRADITIONAL-METHODS"><a href="#A-传统的方法-TRADITIONAL-METHODS" class="headerlink" title="A.传统的方法(TRADITIONAL METHODS)"></a>A.传统的方法(TRADITIONAL METHODS)</h3><p>由于动作识别是时间动作定位的一部分，所以大多数早期动作定位算法都依赖于手动制作的特点，这一点和动作识别相同。这里有几种方式去提取视频特征，包含静态图像特征和时间视觉特征。具体来说，静态图像特征是SIFT (ScaleInvariant Feature Transform，特性变换)和 HOG (Histogram of Oriented Gradients，倾斜的直方图)等待。HOG可以认为是SIFT的一种改进，然而时间视觉特征是静态图像信息和时间信息的结合。通过这些特征，可以得到视频的时间信息。</p>
<p>一般来说，我们能够将特征提取分为局部特征提取和全局特征提取。<strong>a)</strong>局部特征提取是指视频中的局部感兴趣点和感兴趣区域，包括统计数据，字段学习(dictionary learning),bagof-words (BoW),特征学习等等。和全局特征提取想比较，局部特征提取对于视频照明、投是、相机抖动和复杂背景更具有健壮性。<strong>b)</strong>全局特征提取指的是人类行为的整体特征，如人体的轮廓、骨架等，它包括全局密度和轨迹方法。为了解决在复炸场景中人类行为的问题，仅仅检测时空区域灰度变化是不够的。因此，研究人员已经提出了许多依赖于点轨迹的特征提取的方法。近似的过程如下：第一，这些方法先在视频的时间区域检测特征点，然后一帧一帧跟踪这些特征点，并将形成的特征点的轨迹连接起来，最后它们使用特征描述器(feature descriptors)来描述这个轨迹和它的时间域。许多特征提取的方法依赖特征点的轨迹传统方法是Dense Trajectories (DT)，随后，考虑到摄像机的运动导致DT提取的特征和人类行为不相关关联，DT特征提取被更深的改进了，一种叫iDT的方法被提出。iDT的非常有价值的见解(valuable insight)仍然影响着以后的研究工作。值得注意的是深度学习和iDT的结合通常可以更进一步的提高性能。许多论文已经采纳以”我们的方法+iDT”的形式去实现最高水平SOTA(state-of-the-Art)</p>
<p>  无论如何，传统特征提取方法的研究过程和思想非常有用，因为这些方法具有很强的可解释性。它们为设计深度学习方法来解决此类问题提供了启发和类比。</p>
<h3 id="B-深度学习的方法-DEEP-LEARNING-METHODS"><a href="#B-深度学习的方法-DEEP-LEARNING-METHODS" class="headerlink" title="B.深度学习的方法(DEEP LEARNING METHODS)"></a>B.深度学习的方法(DEEP LEARNING METHODS)</h3><p>随着使用手工特征提取的方法的表现变得稳定，时间动作定位以及达到了一定高度。随着卷积神经网络的重生，大量的研究也随之兴起。卷积神经网络可以学习见状的和高水准的特征表示。比如，一个2D-CNN对于一个大规模的视频分类是李菲菲的小组在年提出来的。虽然它的表现和依赖传统方法的特征提取不能相比，但是这个想法启发了后来的研究人员。后来，两种流派CNN((RGB frames and optical flow),  3D卷积神经网络和紧接着它们的变化成为了学习动作识别中的区别性特征受欢迎方法。随后，一个结合两种流派和C3D网络被命名为I3D(Inception 3D)被提出。而且它以及成为了一种通用的视频特征表示编码器(video feature representation encoder)。除了几种依赖神经网络的方法被介绍用来捕捉动态的动作识别，TSN通过稀疏采样的策略，也被设计来对整个视频信息进行平均聚集建模。根据<strong>图1</strong>，我们值得，深度学习分为两种类型：两阶段定位和一阶段定位。</p>
<h4 id="1）两阶段定位方法"><a href="#1）两阶段定位方法" class="headerlink" title="1）两阶段定位方法"></a>1）两阶段定位方法</h4><p>两阶段类型依赖定位-然后-分类( proposal-thenclassification)的范式.这个范式先提取时间定位，然后接着处理分类和回归操作。这方式是主流方法，所以大多数论文都是采用此方法。事实上，这个定位的生成是时间定位范式中的一个难点，这个和目标检测中定位的生成相类似(RNN中区域定位生成)。一个好的定位算法可以更好的提高这个模型的效果</p>
<p>时间动作定位生成器的任务是生成一定数量的时间定位对于一个未裁剪的长视频。一个时间动作定位是一个时间间隔包含动作片段(从起点边界到终点边界)。一般来说，平均召回率用来衡量算法的性能。数据库一般使用ActivityNet和THUMOS14，有几种方法提取定位的方法。</p>
<h5 id="a-滑动窗口-SLIDING-WINDOW-S-CNN-14-2016"><a href="#a-滑动窗口-SLIDING-WINDOW-S-CNN-14-2016" class="headerlink" title="a:滑动窗口(SLIDING WINDOW,S-CNN [14], 2016)"></a>a:滑动窗口(SLIDING WINDOW,S-CNN [14], 2016)</h5><p>在2016年，S-CNN第一个方法是固定一些大小滑动窗口去生成各种大小不同的视频片段，然后通过多级网络(SegmentCNN)处理这些片段。SCNN包括3个子网络都使用C3D网络。第一个是定位(proposal)网络，它用来确定当前路段是一个动作的概率，第二个是分类网络，用来给视频片段分类，第三个是定位(localization)网络,它的输出任然是一个类别的概率。并在训练过程中加入重叠相关的损失函数，使得网络能够更好地估计视频片段的类别喝重叠。原则上，当重叠度越高，效果越好。最后non-maximized suppression (NMS)被用于重复的片段和完成预测。</p>
<p>理论上，只有重叠足够高，这种方法是最全面的，但它有更多的冗余。</p>
<h5 id="b-时间活动分组-TEMPORAL-ACTIONNESS-GROUPING-TAG-15-2017"><a href="#b-时间活动分组-TEMPORAL-ACTIONNESS-GROUPING-TAG-15-2017" class="headerlink" title="b:时间活动分组(TEMPORAL ACTIONNESS GROUPING,TAG [15], 2017)"></a>b:时间活动分组(TEMPORAL ACTIONNESS GROUPING,TAG [15], 2017)</h5><p>以前的工作使用滑动窗口去提取建议的区域，但是这个方法不能处理不同视频动作长度。因为一般的是动作识别，卷积适用于密集视频帧，而且对于长动作视频滑动窗口消耗的资源太多。</p>
<p>Y. Xiong et al在2017年提出了一个新的框架用来准确的确定不同长度的动作视频的边界。这个框架包含两个部分：生成时间区域(generating temporal proposals)和分类待选(classifying proposed candidates)。前一部分生成一系列的建议区域，而后者确定它是否是一个动作并且预测它的类别。未来生成时间建议区域，TAG网络被提出。他们有三个主要步骤：a)提取片段(Extract snippets):每个片段包含一个视频的帧和视觉留信息，而且片段是在一个规律的间隔内获得的。b)动作：判断一个片段是否包含动作，为了做这件事，它使用TSN(Temporal Segment Network)学习二分类网络。c)分组：对于输出的片段序列和他们的概率，它将那些得分较高的连续片段进行分组。与此同时，设置一些阈值去删除分数较低的片段，以防止噪声干扰，并且通常设置多组阈值，以防止缺失建议区域。</p>
<p>这个方法对于边界更加灵活，但是它可能因为分类错误而错过一些建议区域</p>
<h5 id="c-时间单位回归网络-TEMPORAL-UNIT-REGRESS-NETWORK-TURN-TAP-16-2017"><a href="#c-时间单位回归网络-TEMPORAL-UNIT-REGRESS-NETWORK-TURN-TAP-16-2017" class="headerlink" title="c:时间单位回归网络(TEMPORAL UNIT REGRESS NETWORK,TURN TAP [16], 2017)"></a>c:时间单位回归网络(TEMPORAL UNIT REGRESS NETWORK,TURN TAP [16], 2017)</h5><p>在SCNN网络中，它使用滑动窗口去寻找建议区域。如果你想得到准确的结果，你需要增加窗口之间的额重叠，这回导致计算量大的问题。</p>
<p>为了减少计算量和增加时间定位准确度，在2017， Gao J.Y. et al在faster-RCNN映入边界回归方法的基础上，提出了转向学习的方法。这个方法将视频分割成固定大小的单元，列入16帧的单元，然后将每个单元放入C3D种提取水平特征。相邻单元形成一个clip，让么个单位形成一个锚单位，构成一个clip金字塔。然后在单元处进行时间坐标回归，这个网络包含两个输出，第一个输出是确定clip是否包含动作的分数；第二个输出是调整边界的时间坐标偏移量。</p>
<p>这个方法主要的贡献如下：1）一种利用坐标回归生成时间建议分段的新方法。2）快的速度(800fps)。3）一个新的评价指标AR-F被提出</p>
<h5 id="d-边界敏感网络-BOUNDARY-SENSITIVE-NETWORK-BSN-21-2018"><a href="#d-边界敏感网络-BOUNDARY-SENSITIVE-NETWORK-BSN-21-2018" class="headerlink" title="d:边界敏感网络(BOUNDARY SENSITIVE NETWORK ,BSN [21], 2018)"></a>d:边界敏感网络(BOUNDARY SENSITIVE NETWORK ,BSN [21], 2018)</h5><p>众所周知，高质量的时间动作建议区域应该有一下几点特征：a)灵活的时间长度,b)精确的时间界限,c)可靠的信心得分。但是现有的方法不能同时兼顾这些方面，为了解决这些困难，T. Lin et al. [21]提出了BSN在2018年。</p>
<p>简略的说，BSN搜先定位时间动作片段的边界(开始节点和结束节点)。边界节点直接组合成一个时间建议区域。然后根据每个建议区域提案的动作置信度评分序列提取一个32维建议区域特征。最后，根据所提取的建议区域层次特征，对时间提案的置信度进行评估。</p>
<p>这方法主要的贡献如下：a)新颖的框架可以在同一时间满足上诉三点。b)BSN模块简单灵活，缺点是:a)对每个时间方案逐个进行特征提取和置信度评估，效率不够。b)语义信息不足。为了保证动作建议区域特征提取的效率，BSN设计的32维特征相对简单，但也限制了置信度评价模块获取更多的语义信息。c)该方法具有多阶段性。它没有联合优化网络的几个部分。</p>
<h5 id="e-边界匹配网络-BOUNDARY-MATCHING-NETWORK-BMN-72-2019"><a href="#e-边界匹配网络-BOUNDARY-MATCHING-NETWORK-BMN-72-2019" class="headerlink" title="e:边界匹配网络(BOUNDARY-MATCHING NETWORK,BMN [72], 2019)"></a>e:边界匹配网络(BOUNDARY-MATCHING NETWORK,BMN [72], 2019)</h5><p>为了解决BSN的缺点，新的时间建议区域置信度评价机制喝边界匹配机制也由T. Lin在2019年提出。BMN能够生成一维边界概率喝二维BM置信度图。然后对所有可能的时间建议区域进行密集评估。</p>
<p>以上时间动作建议区域方法的性能比较如表1所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201114104053741.png" alt=""></p>
<h4 id="一阶段定位方法"><a href="#一阶段定位方法" class="headerlink" title="一阶段定位方法"></a>一阶段定位方法</h4><p>另一种是同时处理建议区域和分类的一阶段框架。比如，在2017年T. Lin 提出SSAD和李菲菲提出了SS—TAD（端到端，单流时间动作检测）。他们都是基于单个检测器。由于时间动作定位和目标检测相似性，SSAD结合了YOLO和SSD两种目标检测模型的特点。SSAD的一般流程如下，使用提前训练的模型，得到特征序列，作为SSAD模型的输入。模型处理后输出检测结果。而SS-TAD将时间动作定位的语义子任务作为调整语义约束，提高了训练和测试性能。效果优于SSAD，SS-TAD提取特征使用C3D，与SSAD相同。而SS-TAD采用锚定机构和叠加GRU单元。最近，Fuchen Long介绍了GTAN(Gaussian Temporal Awareness Networks,高斯时间感知网络),该网络将时间结构与单阶段动作定位相结合。在GTAN，该算法引入高斯核函数，动态优化每个动作建议的时间尺度。</p>
<p>此外，有些方法基于顺序决策过程，也属于单阶段框架。例如，文献10是第一个提出在视频中学习动作检测的端到端的方法。在这篇文章中,它使用强化学习来训练一个基于rnn的代理。代理可以不断观察视频帧，并决定下一步看哪里，何时生成动作预测</p>
<h2 id="3-基准数据集"><a href="#3-基准数据集" class="headerlink" title="3.基准数据集"></a>3.基准数据集</h2><p>虽然没有一个时间动作定位的标准基准，但大多数研究人员使用THUMOS14[6]和ActivityNet [7]。此外，还有几个大型数据集用于时间动作检测。比如，MEXaction2、MutiTHUMOS、Charades和AVA等。下面这段主要介绍几种常用的数据集</p>
<h3 id="A-THUMOS14"><a href="#A-THUMOS14" class="headerlink" title="A.THUMOS14"></a>A.THUMOS14</h3><p>THUMOS14来自2014年的THUMOS Challenge。这个数据集包含两个任务：动作识别和时间动作检测。大多数论文都在这个数据集中进行评估。THUMOS数据集在其训练、验证和测试集中有101个动作类的视频级注释，而在20个类的验证和测试集中只有一小部分视频有时态注释。</p>
<p>一些全监督学习方法的细节如下:a)训练集:UCF101,101种动作类型，共计修剪了13320个视频剪辑。b)验证集：1010个未修剪的视频，其中200个贴有时间注释。(3007个动作片段，只有20个类可以用于时间动作检测任务)。c)测试集：1574个未修剪的视频，其中213个有时态动作注释。（3358个行为片段，只有20个类可以用于时间动作检测任务）</p>
<p>总之，这个数据集很有挑战性，因为有些视频比较长(长达26分钟)，并且包含多个动作实例。动作的长度在一秒到几分钟之间变化很大。</p>
<h3 id="B-ActivityNet"><a href="#B-ActivityNet" class="headerlink" title="B.ActivityNet"></a>B.ActivityNet</h3><p>ActivityNet数据集是最近引入动作识别和动作定位基准的最大的非裁剪视频数据集。该数据集只提供YouTube视频链接，不能直接下载视频。所以我们需要使用YouTube下载工具自动下载。ActivityNet1.3包含10,024个训练视频，4,926个验证视频，5044个测试视频，200个活动类别，如“遛狗”、“跳远”和“给地板吸尘”。ActivityNet1.3 平均每个视频只包含1.5次出现，大多数视频只包含单个动作类别，平均36%的背景。</p>
<p>这个数据集包含在语义分类下涉及各种人类活动的大量自然视频。</p>
<h3 id="C-MEXaction2"><a href="#C-MEXaction2" class="headerlink" title="C.MEXaction2"></a>C.MEXaction2</h3><p>MEXaction2数据集包含两种类型的动作，即骑马和斗牛。这个数据集由三部分组成:YouTube视频，UCF101中的骑马视频和INA视频。其中UCF101中的YouTube视频片段和骑马视频是裁剪后的短视频片段，用于训练集，而INA视频是未裁剪的长视频，总长度为77小时。INA视频分为培训、验证和测试集。训练集、验证集和测试集中分别有1336、310和329个动作片段。</p>
<p>总之，MEXaction2 dataset的特点是未裁剪的视频非常长，注释片段只占整个视频的很小一部分。</p>
<h3 id="D-MUTITHUMOS"><a href="#D-MUTITHUMOS" class="headerlink" title="D.MUTITHUMOS"></a>D.MUTITHUMOS</h3><p>MUTITHUMOS是一个密集的，多类，帧明智标签视频数据集，包括400个视频30小时，38,690个65个类的注释。平均每帧有1.5个标签，每段视频有10.5个动作类。这是THUMOS的增强版。目前，我们只在2017的论文“‘Learning Latent Super-Events to Detect Multiple Activities in Videos”中看到对该数据集的评价</p>
<h3 id="E-CHARADES"><a href="#E-CHARADES" class="headerlink" title="E.CHARADES"></a>E.CHARADES</h3><p>Charades是未删减的视频，其中包含9,848个室内视频，(7985个训练数据，1863年测试数据)，以及来自267个不同人的157个课程。每个视频大约30秒。每个视频都有多个注释，每个动作的开始时间和结束时间。</p>
<h3 id="F-AVA"><a href="#F-AVA" class="headerlink" title="F.AVA"></a>F.AVA</h3><p>AVA是一个时空本地化的原子视觉动作数据集。它包含了430个15分钟长的电影片段，并注释了80个动作。有38.6万个标记片段，61.4万个标记包围框和8.1万个人员轨迹。总共有158万标记的行为，每个人经常有多个标记。</p>
<p>接下来，我们总结和比较这些数据集显示表2。</p>
<p><img src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201114213744116.png" alt=""></p>
<h2 id="4-评价指标"><a href="#4-评价指标" class="headerlink" title="4.评价指标"></a>4.评价指标</h2><h3 id="A-基本概念"><a href="#A-基本概念" class="headerlink" title="A.基本概念"></a>A.基本概念</h3><p>在二分类问题当中，TP代表True Positive，FP代表False Positive，TN代表True Negative，而FN代表False Negative。这四个参数被用来计算多种性能评价指标。给出了四个参数的逻辑细节在表3中。</p>
<p><img src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201114214140041.png" alt=""></p>
<p>其中，在实际的二进制分类中，positive-1标签指的是你比较关心的样本，比如一个动作或者一个异常事件。</p>
<h4 id="1-ACCURACY"><a href="#1-ACCURACY" class="headerlink" title="1)ACCURACY"></a>1)ACCURACY</h4><p>准确度是指正确分类样本的比例。它被用来评价分级机的性能。</p>
<script type="math/tex; mode=display">
accuracy=\frac{rP+TN}{TP + TN + FP + FN}=\frac{TP+TN}{ALL}(1)</script><h4 id="2-RECALL"><a href="#2-RECALL" class="headerlink" title="2)RECALL"></a>2)RECALL</h4><p>召回率是正确预测的范围。准确的说，找回是测试集中确定了多少真的positive样本。它的公式如下。</p>
<script type="math/tex; mode=display">
recall=\frac{TP}{TP+FN}（2）</script><h4 id="3-PRECISION"><a href="#3-PRECISION" class="headerlink" title="3)PRECISION"></a>3)PRECISION</h4><p>准确的说，准度是预测真实positive样本占预测结果的百分比。它的公式如下。</p>
<script type="math/tex; mode=display">
Precision=\frac{TP}{TP+FP}=\frac{TP}{n}(3)</script><p>其中，n为真positive和假positive之和，n为系统识别的样本总数。</p>
<h4 id="4-INTERSECTION-OVER-UNION-IoU"><a href="#4-INTERSECTION-OVER-UNION-IoU" class="headerlink" title="4)INTERSECTION-OVER-UNION (IoU)"></a>4)INTERSECTION-OVER-UNION (IoU)</h4><p>IoU可以理解为模型预测的检测框与图像中目标检测的地面真实值的重叠。其实就是检测的准确性，计算公式为探测结果与地面真值的交点，并与两者的联合进行比较。</p>
<script type="math/tex; mode=display">
IoU=\frac{predicted detection box\cap ground truth}{predicted detection box\cup ground truth}(4)</script><p>IoU用于检查预测结果与地面真实值之间的IoU是否大于预测阈值。我们通常把0.5作为阈值。如果IoU大于0.5，则该对象被识别为“检测成功”，否则被识别为“漏报”。在时间动作检测中，将IoU转化为时间的t-IoU，该t-IoU只有一维。</p>
<h3 id="B-评价指标"><a href="#B-评价指标" class="headerlink" title="B. 评价指标"></a>B. 评价指标</h3><h3 id="C-平均召回-AVERAGE-RECALL-AR"><a href="#C-平均召回-AVERAGE-RECALL-AR" class="headerlink" title="C.平均召回(AVERAGE RECALL,AR)"></a>C.平均召回(AVERAGE RECALL,AR)</h3><p>AR是时间动作建议区域生成的评价指标。因为时间动作建议区域生成不需要分类，它仅仅只需要去发现建议区域。因此，我们发现的时间动作建议区域是否完整，可以用来评估该方法的性能。所有我们经常使用AR来对此进行判断</p>
<script type="math/tex; mode=display">
AR=\frac{sum of the videos recalled}{total number of videos}(5)</script><h3 id="D-均值平均精度-MEAN-AVERAGE-PRECISION-mAP"><a href="#D-均值平均精度-MEAN-AVERAGE-PRECISION-mAP" class="headerlink" title="D. 均值平均精度(MEAN AVERAGE PRECISION,mAP)"></a>D. 均值平均精度(MEAN AVERAGE PRECISION,mAP)</h3><p>在时间动作定位任务中，mAP是最常用的评价指标。一般来说，我们在t-IoU = 0.5的情况下比较mAp。</p>
<p>简单地说， Precision (P)是给定视频中单个类的正确检测程度。例如，对于给定的单个视频，在公式中显示了类C的精度。</p>
<script type="math/tex; mode=display">
P=\frac{TP}{TP + FP}=\frac{number of predicted correct proposals}{total mumber of predicted proposals}(6)</script><p>由于测试集中有很多视频，平均精度(AP)是类C中所有视频的平均精度。同时，由于测试集视频对应的类也很多，所以平均平均精度就是所有测试视频中所有类的平均精度。</p>
<script type="math/tex; mode=display">
mAP=\frac{the sum of average precision of all classes}{total number of videos in testing set}(7)</script><p>总之，在某t-IoU下，P是某类C在视频中预测建议区域的准确性。AP是视频中所有类别的预测建议的平均精度。MAP是所有测试视频中所有类的预测建议的平均精度的平均值。在标准评价方案下，几乎所有的文献都报道了不同阈值的t-IoU下的mAP</p>
<h2 id="5-最近的方法和发展"><a href="#5-最近的方法和发展" class="headerlink" title="5.最近的方法和发展"></a>5.最近的方法和发展</h2><h3 id="A-全监督时间动作定位-FULLY-SUPETVISED-TEMPORAL-ACTION-LOCALIZATION-F-TAL"><a href="#A-全监督时间动作定位-FULLY-SUPETVISED-TEMPORAL-ACTION-LOCALIZATION-F-TAL" class="headerlink" title="A.全监督时间动作定位(FULLY-SUPETVISED TEMPORAL ACTION LOCALIZATION,F-TAL)"></a>A.全监督时间动作定位(FULLY-SUPETVISED TEMPORAL ACTION LOCALIZATION,F-TAL)</h3><h4 id="1-全监督学习"><a href="#1-全监督学习" class="headerlink" title="1)全监督学习"></a>1)全监督学习</h4><p>==全监督学习是训练一种智能算法将输入数据映射到标签的过程，其中每个训练数据都有其对应的表示其ground truth的标签==。我们经常学习的分类和回归是监督式学习的代表。在时间动作定位任务中，全监督采用训练集的标签，该标签既包含视频级类别标签，又包含动作片段的时间标注信息(包括动作的开始时间和结束时间)。</p>
<h4 id="2-目前具有代表性的方法"><a href="#2-目前具有代表性的方法" class="headerlink" title="2)目前具有代表性的方法"></a>2)目前具有代表性的方法</h4><p>很多方法(如S-CNN[14]和PSDF[18])通过滑动窗口生成建议区域，并将提案分类为C + 1类，即C动作类和一个背景类。其中，S-CNN使用了多级CNN时间动作定位捕获鲁棒的视频特征表示。为了精确的边界，CDC [19] (Convolutional- de -Convolutional)网络和TPC-Net [20] ((Temporal Preservation Convolutional network)网络被提出用于帧级的动作预测。边界敏感网络(Boundary Sensitive Network, BSN[21])是最近提出的一种用于定位时间边界的网络，并进一步整合到动作建议区域中。次年，BSN的作者提出了一个新的时间建议区域置信度评价机制和边界匹配机制BMN [72]。BMN可以同时生成一维边界概率和二维BM的置信度图。为了保证建议区域的完整性，SSN[22]引入了结构化时间金字塔，利用解耦的分类器对行动进行分类和确定完整性。此外，一些基于区域的方法(如R-C3D[23]和talo - net[24])提出将二维目标检测方法推广到一维时间动作定位。最近，为了精确动作定位而提出了TSA-Net [26]和Gaussian temporal modeling [25]。下面是性能比较。为了简单和公平，我们在THUMOS14数据集和各种代表性方法的出版物上对mAP@tIoU = 0.5进行性能比较，如表4所示。</p>
<p>近年来，随着各种新型网络的引进，准确率已达到最新的46.9%[26]。当然，与图像中的目标检测还有一定的差距，这也是目前难以实现大规模商业化的原因。但我们可以相信，随着技术的不断进步，精度一定会实现突破。</p>
<p><img src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201115121823022.png" alt=""></p>
<h3 id="B-弱监督时间动作定位-W-TAL"><a href="#B-弱监督时间动作定位-W-TAL" class="headerlink" title="B.弱监督时间动作定位(W-TAL)"></a>B.弱监督时间动作定位(W-TAL)</h3><p>由上节可知，目前的全监督学习技术在时间动作定位方面取得了很大的成功。因为许多现有的技术依赖于裁剪后的视频作为输入，例如：UCF101，他们有这些精确的时间注释。但在现实情况下，大多数视频都是未修剪并包含许多与目标动作无关的帧。因此，对时态标注的要求是非常困难的。具体原因总结如下:a)每个动作实例的帧级注释既昂贵又耗时。b)时间行为并没有明确的定义，这些时间行为的标注可能是因人而异的。</p>
<p>因此，弱监督学习方法越来越受欢迎。</p>
<h4 id="1-弱监督学习"><a href="#1-弱监督学习" class="headerlink" title="1)弱监督学习"></a>1)弱监督学习</h4><p>让我们看一看弱监督学习。这里有三种类型的弱监督学习。a)不完全弱监督学习。不完全监督，这只是训练数据的一小部分标记，而其他数据没有标记。例如，在图像分类中，我们可以很容易地从网上得到大量的图像，但由于人工成本昂贵，只有少数图像有注释。b)不精确弱监督学习。也就是说训练数据只有粗粒度的标签。我们还以图像分类为例。我们通常有图像级标签，但没有对象级标签。c)不准确弱监督学习。也就是说，给我们的标签并不总是真实的。例如，当图像注释器疲劳或者粗心大意的时候，或者一些图像很难分类的时候，就会出现这种情况。</p>
<p>由上可知，弱监督时间定位在训练过程中只有视频级标签，没有帧级时间标注，属于第二类弱监督，即不精确监督。</p>
<h4 id="2-目前具有代表性的方法-1"><a href="#2-目前具有代表性的方法-1" class="headerlink" title="2)目前具有代表性的方法"></a>2)目前具有代表性的方法</h4><p>目前基于弱监督的定位方法很少，仅依赖于视频级的类标签来实现时间动作定位。受图像中弱监督目标检测的启发，研究人员对UntrimmedNet[29]和Hide-and-seek [30]进行了研究。UntrimmedNet是第一个提出动作识别和弱监督动作检测的公司。它是学习单标签动作分类和检测的一个端到端模型。STPN [31]是一个深度神经网络，依赖于分类。这个网络的总体结构如下:将视频分为N个片段，注意力模块可以识别关键片段的稀疏子集。然后我们就可以得到在分类标签预测过程中各个环节的重要性。因此，它能够通过自适应时间池化生成相应的类别标签和区间建议。AutoLoc [32]尝试根据CAS的阈值直接预测不同于以往弱监督时间动作检测的时间边界。==其主要思想是鼓励动作部分外的平均分低于动作类内的平均分==。W-TALC [33] 引入一种新的函数来进行K-max多实例学习，挖掘同一类局部实例之间的协同活动关系。为了解决分类器所关心的视频帧的碎片化和动作的完整性问题，Hide-and-seek[30]随机隐藏一些帧，迫使剩余注意力在每次训练中学习辨别度较低的视频帧。虽然，它不能保证在每次训练中都能发现新零件。结果表明，该方法对空间目标检测效果良好，但对时间动作定位效果不佳。Step-by-step erasion, one-by-one collection 对多个分类器进行擦除和训练，直接合并每个分类器的预测片段。性能好了一些，但是它花费更多的时间和计算。随后，CMCS[35]提出了一种具有多样性损失的多分支网络结构，用于动作完整性建模。与此同时，他们提出了一个方案，生成一个hard negative 视频来分离上下文。虽然本文的重点不是背景类，但它启发了接下来的三个作品，即BaSNet[36]，background modeling[37]和LPAT [38]。在没有考虑背景类别的情况下，将背景帧误分类为动作类别，导致大量FPs。在BasNet中，为了构造背景类的负样本，在另一个网络中引入注意模块来抑制背景响应。另外两部作品从不同的角度考虑背景阶级，有效地抑制了背景的影响。最后，它们都提高了定位的准确性。</p>
<p>以下是在THUMOS14数据集上的性能比较，与全监督学习和各种代表性方法发表的标准相同，如表5所示</p>
<p><img src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/paper/image-20201115130532292.png" alt=""></p>
<h4 id="3）对W-TAL问题的见解"><a href="#3）对W-TAL问题的见解" class="headerlink" title="3）对W-TAL问题的见解"></a>3）对W-TAL问题的见解</h4><p>近年来，多用例学习(multiple instance learning, MIL)被应用于W-TAL学习中。MIL模型不是使用一组单独标记的实例来学习，而是接收一组标记的包，每个包包含许多实例。如果我们把视频中的动作实例看作一个包，把视频级别的注释看作标签，那么W-TAL可以被表述为一个多实例学习的过程。</p>
<p>时序类激活映射(Temporal class activation mapping，T-CAM)或类激活序列(r Class activation sequence ，CAS)是近年来出现的另一种W-TAL方法。CNN可视化显示，CNN的卷积层作为动作探测器执行，尽管对活动的位置没有监督。类激活序列说明，CNN尽管接受过视频层次标签的训练，但仍然能够具有本地化能力。此外，基于弱监督目标检测的其他研究，如交互式标注和生成对抗训练，也对W-TAL的研究有所启发。</p>
<p>综上所述，弱监督学习降低了劳动和时间成本，但也增加了时间检测的难度。但对于大多数动作类的大多数视频剪辑来说，效果似乎还不错。当然，仍有很大的改进空间</p>
<h2 id="5-未来方向和发展趋势"><a href="#5-未来方向和发展趋势" class="headerlink" title="5.未来方向和发展趋势"></a>5.未来方向和发展趋势</h2><p>时间动作定位的应用实际上会越来越广泛，未来的发展趋势可能集中但不限于以下几方面</p>
<p>a)精度和效率的改进。通过与二维卷积法和三维卷积法的比较，二维卷积法的精度更高，但效率较低。如何更好地发挥二者的优势是一个可能的研究方向</p>
<p>b)动作检测将从时间动作检测扩展到时空动作检测[39]。也就是说，我们应该从一维的时间间隔检测到二维的时空盒，这样可以更全面的检测动作</p>
<p>c)在线视频的动作检测。这是一个处理视频流的过程，需要在线检测动作的类别，但检测时间过后无法知道内容。在线的设置更符合需要实时检测或预警的监控视频的要求，如异常检测[41]。</p>
<p>d)时间动作定位的弱监督学习将会越来越受欢迎。在许多任务中，由于数据标注过程成本高昂，难以获得完整的监管信息。</p>
<p>e)视频是一种包含图像和音频的多模态数据。是否利用音频信息辅助时间动作定位是值得考虑的方向。正如Aytar等人使用图像辅助音频分析[45]。</p>
<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6.结论"></a>6.结论</h2><p>在本文中，我们对时间动作定位进行了全面的概述。我们从时间划分的角度分析了相关技术:传统方法和深度学习方法。接下来我们总结基准数据集和分析评估指标。然后回顾了时间动作定位从完全监督学习到弱监督学习方法的最新进展。总之，我们试图给出时间行为本土化的相关性和现状。同时，我们也希望对时间动作定位感兴趣的读者有所帮助。</p>
<p>时间动作定位是视频理解领域的一个研究热点，具有很大的复杂性和挑战性。但是，我们相信在不久的将来，深度学习技术的应用可以改善结果，任务也会变得更加容易。</p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E6%97%B6%E5%BA%8F%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D/" rel="tag"><i class="fa fa-tag"></i> 时序动作定位</a>
              <a href="/tags/%E5%8A%A8%E4%BD%9C%E6%A3%80%E6%B5%8B/" rel="tag"><i class="fa fa-tag"></i> 动作检测</a>
              <a href="/tags/%E7%BB%BC%E8%BF%B0/" rel="tag"><i class="fa fa-tag"></i> 综述</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/05/30/%E5%85%B6%E4%BB%96/hexo%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/" rel="prev" title="hexo使用github action实现自动化部署">
      <i class="fa fa-chevron-left"></i> hexo使用github action实现自动化部署
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/06/15/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/Learning%20Temporal%20Co-Attention%20Models%20for%20Unsupervised%20Video%20Action/" rel="next" title="Learning Temporal Co-Attention Models for Unsupervised Video Action Localization">
      Learning Temporal Co-Attention Models for Unsupervised Video Action Localization <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#A-Survey-on-Temporal-Action-Localization"><span class="nav-number">1.</span> <span class="nav-text">A Survey on Temporal Action Localization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%BC%95%E8%A8%80"><span class="nav-number">1.1.</span> <span class="nav-text">1.引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF"><span class="nav-number">1.2.</span> <span class="nav-text">2.相关技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%96%B9%E6%B3%95-TRADITIONAL-METHODS"><span class="nav-number">1.2.1.</span> <span class="nav-text">A.传统的方法(TRADITIONAL METHODS)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95-DEEP-LEARNING-METHODS"><span class="nav-number">1.2.2.</span> <span class="nav-text">B.深度学习的方法(DEEP LEARNING METHODS)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%EF%BC%89%E4%B8%A4%E9%98%B6%E6%AE%B5%E5%AE%9A%E4%BD%8D%E6%96%B9%E6%B3%95"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">1）两阶段定位方法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#a-%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3-SLIDING-WINDOW-S-CNN-14-2016"><span class="nav-number">1.2.2.1.1.</span> <span class="nav-text">a:滑动窗口(SLIDING WINDOW,S-CNN [14], 2016)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#b-%E6%97%B6%E9%97%B4%E6%B4%BB%E5%8A%A8%E5%88%86%E7%BB%84-TEMPORAL-ACTIONNESS-GROUPING-TAG-15-2017"><span class="nav-number">1.2.2.1.2.</span> <span class="nav-text">b:时间活动分组(TEMPORAL ACTIONNESS GROUPING,TAG [15], 2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#c-%E6%97%B6%E9%97%B4%E5%8D%95%E4%BD%8D%E5%9B%9E%E5%BD%92%E7%BD%91%E7%BB%9C-TEMPORAL-UNIT-REGRESS-NETWORK-TURN-TAP-16-2017"><span class="nav-number">1.2.2.1.3.</span> <span class="nav-text">c:时间单位回归网络(TEMPORAL UNIT REGRESS NETWORK,TURN TAP [16], 2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-%E8%BE%B9%E7%95%8C%E6%95%8F%E6%84%9F%E7%BD%91%E7%BB%9C-BOUNDARY-SENSITIVE-NETWORK-BSN-21-2018"><span class="nav-number">1.2.2.1.4.</span> <span class="nav-text">d:边界敏感网络(BOUNDARY SENSITIVE NETWORK ,BSN [21], 2018)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#e-%E8%BE%B9%E7%95%8C%E5%8C%B9%E9%85%8D%E7%BD%91%E7%BB%9C-BOUNDARY-MATCHING-NETWORK-BMN-72-2019"><span class="nav-number">1.2.2.1.5.</span> <span class="nav-text">e:边界匹配网络(BOUNDARY-MATCHING NETWORK,BMN [72], 2019)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%80%E9%98%B6%E6%AE%B5%E5%AE%9A%E4%BD%8D%E6%96%B9%E6%B3%95"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">一阶段定位方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.3.</span> <span class="nav-text">3.基准数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-THUMOS14"><span class="nav-number">1.3.1.</span> <span class="nav-text">A.THUMOS14</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-ActivityNet"><span class="nav-number">1.3.2.</span> <span class="nav-text">B.ActivityNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-MEXaction2"><span class="nav-number">1.3.3.</span> <span class="nav-text">C.MEXaction2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-MUTITHUMOS"><span class="nav-number">1.3.4.</span> <span class="nav-text">D.MUTITHUMOS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#E-CHARADES"><span class="nav-number">1.3.5.</span> <span class="nav-text">E.CHARADES</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#F-AVA"><span class="nav-number">1.3.6.</span> <span class="nav-text">F.AVA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">1.4.</span> <span class="nav-text">4.评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.4.1.</span> <span class="nav-text">A.基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-ACCURACY"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">1)ACCURACY</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-RECALL"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">2)RECALL</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-PRECISION"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">3)PRECISION</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-INTERSECTION-OVER-UNION-IoU"><span class="nav-number">1.4.1.4.</span> <span class="nav-text">4)INTERSECTION-OVER-UNION (IoU)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">1.4.2.</span> <span class="nav-text">B. 评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#C-%E5%B9%B3%E5%9D%87%E5%8F%AC%E5%9B%9E-AVERAGE-RECALL-AR"><span class="nav-number">1.4.3.</span> <span class="nav-text">C.平均召回(AVERAGE RECALL,AR)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#D-%E5%9D%87%E5%80%BC%E5%B9%B3%E5%9D%87%E7%B2%BE%E5%BA%A6-MEAN-AVERAGE-PRECISION-mAP"><span class="nav-number">1.4.4.</span> <span class="nav-text">D. 均值平均精度(MEAN AVERAGE PRECISION,mAP)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%9C%80%E8%BF%91%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E5%8F%91%E5%B1%95"><span class="nav-number">1.5.</span> <span class="nav-text">5.最近的方法和发展</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-%E5%85%A8%E7%9B%91%E7%9D%A3%E6%97%B6%E9%97%B4%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D-FULLY-SUPETVISED-TEMPORAL-ACTION-LOCALIZATION-F-TAL"><span class="nav-number">1.5.1.</span> <span class="nav-text">A.全监督时间动作定位(FULLY-SUPETVISED TEMPORAL ACTION LOCALIZATION,F-TAL)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%85%A8%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">1)全监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%9B%AE%E5%89%8D%E5%85%B7%E6%9C%89%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">2)目前具有代表性的方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#B-%E5%BC%B1%E7%9B%91%E7%9D%A3%E6%97%B6%E9%97%B4%E5%8A%A8%E4%BD%9C%E5%AE%9A%E4%BD%8D-W-TAL"><span class="nav-number">1.5.2.</span> <span class="nav-text">B.弱监督时间动作定位(W-TAL)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">1)弱监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E7%9B%AE%E5%89%8D%E5%85%B7%E6%9C%89%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%9A%84%E6%96%B9%E6%B3%95-1"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">2)目前具有代表性的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3%EF%BC%89%E5%AF%B9W-TAL%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%81%E8%A7%A3"><span class="nav-number">1.5.2.3.</span> <span class="nav-text">3）对W-TAL问题的见解</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91%E5%92%8C%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF"><span class="nav-number">1.6.</span> <span class="nav-text">5.未来方向和发展趋势</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E7%BB%93%E8%AE%BA"><span class="nav-number">1.7.</span> <span class="nav-text">6.结论</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="bugCat"
      src="https://cdn.jsdelivr.net/gh/bugcat9/blog-image-bed@main/avatar/zhouning.png">
  <p class="site-author-name" itemprop="name">bugCat</p>
  <div class="site-description" itemprop="description">啥都不会</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">95</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">43</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/bugcat9" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;bugcat9" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1767508581@qq.com" title="E-Mail → mailto:1767508581@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">bugCat</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">493k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:28</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '5181a7c6b299b2f49124',
      clientSecret: '840df9eaf797888f8029b5a03aca0f28fe042bcf',
      repo        : 'bugcat9.github.io',
      owner       : 'bugcat9',
      admin       : ['bugcat9'],
      id          : 'c3e69491f70a10e7a7b5ce6326f3ea0c',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

    </div>
  

  <script async src="/js/cursor/fireworks.js"></script>




  <script src="/js/activate-power-mode.min.js"></script>
  <script>
    POWERMODE.colorful = true;
    POWERMODE.shake = false;
    document.body.addEventListener('input', POWERMODE);
  </script>

</body>
</html>
